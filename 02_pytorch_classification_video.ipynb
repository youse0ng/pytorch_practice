{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3MEg4sgnXZ5bTxTzwrzrC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youse0ng/pytorch_practice/blob/main/02_pytorch_classification_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02. Neural Network classification with Pytorch\n",
        "\n",
        "Classification is a problem of predicting whether something is one thing or another (there can be multiple things as the options).\n",
        "분류는 분류(Classification)는 어떤 것이 다른 것 중 하나인지 예측하는 문제입니다."
      ],
      "metadata": {
        "id": "bJVRwchrGB5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Make classification data and get it ready"
      ],
      "metadata": {
        "id": "xAgmooL8vp70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn"
      ],
      "metadata": {
        "id": "JxzdCWXmv0qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# Make 1000 samples\n",
        "n_samples=1000\n",
        "\n",
        "# Create circles (넘파이 어레이)\n",
        "X,y= make_circles(n_samples,\n",
        "                  noise=0.03,\n",
        "                  random_state=42)"
      ],
      "metadata": {
        "id": "L3x8BBNavyBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[0],y[0]"
      ],
      "metadata": {
        "id": "vDnAbZk_wOl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First 5 samples of X:\\n {X[:5]}\")\n",
        "print(f\"First 5 samples of y:\\n {y[:5]}\")"
      ],
      "metadata": {
        "id": "D2NPlRS_wSw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make dataframe of circle data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "circles=pd.DataFrame({\"X1\":X[:,0],\n",
        "                      \"X2\":X[:,1],\n",
        "                      \"label\":y})\n",
        "\n",
        "circles.head(10)"
      ],
      "metadata": {
        "id": "KbF6Hc6Gw3vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "circles.label.value_counts()\n",
        "# 1번 클래스 500개\n",
        "# 0번 클래스  500개"
      ],
      "metadata": {
        "id": "ngGZ1YGSep-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize,visualize,visualize\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x=X[:,0],\n",
        "            y=X[:,1],\n",
        "            c=y, # color\n",
        "            cmap= plt.cm.RdYlBu # 컬러)\n",
        ")\n"
      ],
      "metadata": {
        "id": "IunIrF5wxoSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** The data we're working with is often referred to as a toy dataset, a dataset that is small enough to experiment but still sizeable enough to practice the fundamental"
      ],
      "metadata": {
        "id": "sry5da4byaS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Check input and output shapes"
      ],
      "metadata": {
        "id": "oeKTzdR-12xP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape,y.shape"
      ],
      "metadata": {
        "id": "esGY7waX17zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first example of features and labels\n",
        "\n",
        "X_sample=X[0]\n",
        "y_sample=y[0]\n",
        "\n",
        "print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\n",
        "print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")\n",
        "\n",
        "# 스칼라는 형태가 없어서 ()로 출력"
      ],
      "metadata": {
        "id": "ebqs7Piv6w8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21_s9fZaii9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Turn data into tensors and create train and test splits"
      ],
      "metadata": {
        "id": "sDnle3weLTU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "xawj97k1LZSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X), X.dtype\n",
        "# 넘파이의 기본 데이터형은 float 64이다\n",
        "# 파이토치 기본 유형은 float 32이다\n",
        "# X는 넘파이 어레이이다."
      ],
      "metadata": {
        "id": "PY9eMMIhMEUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn data into tensors\n",
        "X=torch.from_numpy(X).type(torch.float) # 파이토치 기본형으로 변환\n",
        "y=torch.from_numpy(y).type(torch.float) # 파이토치 기본형으로 변환\n",
        "\n",
        "X[:5],y[:5]"
      ],
      "metadata": {
        "id": "Y-H2NUGfL1P3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X),X.dtype,y.dtype"
      ],
      "metadata": {
        "id": "QKOveNXAU6gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,\n",
        "                                               y,\n",
        "                                               test_size=0.2, # 0.2=20% of data will be test & 80% data will be train\n",
        "                                               random_state=42 # torch.manual_seed(42)와 유사\n",
        ")"
      ],
      "metadata": {
        "id": "RwvDXp4xVGPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train),len(X_test), len(y_train),len(y_test)\n",
        "\n",
        "# 훈련용 샘플 800 ,\n",
        "# 테스트 샘플 200\n"
      ],
      "metadata": {
        "id": "p8JVJN9gVSDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0],y_train[0]"
      ],
      "metadata": {
        "id": "Trij-ksRipfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples # 1000"
      ],
      "metadata": {
        "id": "dF07Gz75WW8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 데이터를 확인해보기 -> indexing 으로 확인\n",
        "\n",
        "2. 데이터를 가공 (dataframe화) = DataFrame\n",
        "\n",
        "https://kongdols-room.tistory.com/107\n",
        "\n",
        "Pandas에서 데이터를 확인하는 방법 중 하나로, 가장 쉽고 단순하게 확인할 수 있는 함수\n",
        "* head()함수, head(n=5), DataFrame 내의 처음 n줄의 데이터를 출력\n",
        "\n",
        "* tail()함수, tail(n=5),DataFrame 내의 마지막 n줄의 데이터를 출력한다.\n",
        "\n",
        "객체 내에 데이터 타입을 확인하는데 유용\n",
        "\n",
        "그래도 만약 데이터가 뭔지 모르겠다면 Visualize!\n",
        "\n",
        "3. import matplotlib.pyplot as plt\n",
        "\n",
        "scatter() 메서드에서 cmap은 컬러 맵(colormap)을 지정하는 매개변수입니다. 컬러 맵은 데이터 포인트의 값에 따라 색상을 매핑하는 데 사용됩니다. cmap 매개변수를 통해 지정된 컬러 맵은 색상 매핑에 사용되며, 각 데이터 포인트에 대한 색상을 결정합니다.\n",
        "\n",
        "컬러 맵은 주로 연속적인 데이터에 사용되며, 데이터의 범위에 따라 다양한 색상으로 표현할 수 있습니다. 예를 들어, 데이터가 낮은 값부터 높은 값까지 범위를 가지는 경우, 컬러 맵은 해당 범위에 대한 색상 그라디언트를 생성합니다.\n",
        "\n",
        "c(color)는\n",
        "\n",
        "scatter(c=y)는 scatter() 메서드에서 c 매개변수에 y 값을 전달하는 것을 의미합니다. 이 경우 y 값이 데이터 포인트의 색상을 지정하는 데 사용됩니다.\n",
        "\n",
        "일반적으로 c 매개변수에 단일 값, 값의 리스트 또는 배열, 또는 컬러 맵을 지정할 수 있습니다. c=y의 경우 y는 데이터 포인트의 색상을 나타내는 값들의 리스트 또는 배열입니다.\n",
        "\n",
        "scatter() 메서드에 c=y를 사용하여 데이터 포인트의 색상을 y 값에 따라 지정할 수 있습니다\n",
        "\n",
        "c=y를 사용하여 데이터 포인트의 색상을 y 값에 따라 지정하였습니다. y 값이 색상으로 사용되므로 y 값이 클수록 더 진한 색상으로 표시됩니다.\n",
        "\n",
        "4. 데이터 split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "사용"
      ],
      "metadata": {
        "id": "8Ph9sqEXWgUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a model\n",
        "\n",
        "Let's build a model to classify our blue and red dots.\n",
        "\n",
        "To do so, we want to:\n",
        "1. Setup device agnostic code so our code will run on an accelerator(GPU) if there is one\n",
        "2. Construct a model (by subclassing 'nn.Module')\n",
        "3. Define a loss function and optimizer\n",
        "4. Create a training loop and test loop"
      ],
      "metadata": {
        "id": "lr0bBMYUAptu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pytorch and nn\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Make device agnostic code\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "8IND4mfcH__v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "03AzQNYLMC6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've setup device agnostic code, let's create a model that:\n",
        "\n",
        "1. Subclass `nn.Module`(almost all models in Pytorch subclass `nn.Module`)\n",
        "2. Create 2 `nn.Linear()` layers that are capable of handling the shapes of our data\n",
        "3. Defines a `forward()` method that outlines the forward pass(or forward computation) of our model\n",
        "4. Instatiate an instance of our model class and send it to the target `device`"
      ],
      "metadata": {
        "id": "rBc-vnDuMtoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "tlxQ8SplOP6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Construct a model that subclasses nn.Module\n",
        "\n",
        "class CirclemodelV0(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # 2. Create 2 nn.Linear layers capable of handling the shapes of our data\n",
        "    #shift command space -> 독스트링이 나옴\n",
        "    self.layer_1=nn.Linear(in_features=2 ,out_features=5)  # 첫 레이어의 in_feature가 왜 2일까 그건 X_train.shape를 보면 알 수 있다.\n",
        "    self.layer_2=nn.Linear(in_features=5,out_features=1) # output layer #in_feature의 value는 이전 레이어의 out_feature의 value와 같아야한다. 따라서 5가 된다.\n",
        "    # in_feature의 value가 이전 레이어의 out_feature와 맞지 않는다면 모양 불일치 오류가 난다.\n",
        "    # 8의 배수가 컴퓨팅에 좋다. 경험에 의거한\n",
        "    # 경험에 따르면 숨겨진 hidden feature 가 많으면 모델이 데이터의 패턴을 학습할 기회가 많아진다.\n",
        "    # 위에서 패턴을 학습할 수 있는 숫자가 2개 (X1,X2), 5로 업스케일링 하면 패턴을 배울 수 있는 숫자가 5개가 된다.\n",
        "\n",
        "  # 3. Define a forward() method that outlines the forward pass\n",
        "  def forward(self, x):\n",
        "    return self.layer_2(self.layer_1(x)) # x -> layer_1 ->layer_2 -> return output\n",
        "\n",
        "  # 4. Instatiate an instance of our model class and send it to the target device\n",
        "model_0=CirclemodelV0().to(device)\n",
        "model_0"
      ],
      "metadata": {
        "id": "fdsfUB1iM0MB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequential 을 이용한 모델 정의\n",
        "class CirclemodelV1_sequential(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.two_linear_layers=nn.Sequential(\n",
        "        nn.Linear(in_features=2,out_features=5),\n",
        "        nn.Linear(in_features=5,out_features=1)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.two_linear_layers(x)\n",
        "\n",
        "model_sequential=CirclemodelV1_sequential()\n",
        "\n",
        "model_sequential.state_dict()"
      ],
      "metadata": {
        "id": "ete8wk_LTbr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(model_0.parameters()).device # 모델의 파라미터를 cuda에다 저장\n",
        "#next() 메소드는 파이썬에서 이터레이터(iterator)를 사용할 때 다음 요소를 반환하는 함수입니다.\n",
        "#이터레이터는 값을 한 번에 하나씩 차례대로 반환하는 객체로,\n",
        "#반복 가능한(iterable) 객체를 대표하는 인터페이스입니다.\n",
        "\n",
        "#my_list = [1, 2, 3, 4, 5]\n",
        "#my_iterator = iter(my_list)\n",
        "#print(next(my_iterator))  # 1\n",
        "#print(next(my_iterator))  # 2\n",
        "#print(next(my_iterator))  # 3\n"
      ],
      "metadata": {
        "id": "SQD39L-Iy33N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensorflow playground\n",
        "----\n",
        "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.35820&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
      ],
      "metadata": {
        "id": "_Zc62jAb9YHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's replicate the model above using nn.Sequential()\n",
        "\n",
        "model_0=nn.Sequential(\n",
        "    nn.Linear(in_features=2,out_features=5),\n",
        "    nn.Linear(in_features=5,out_features=1)\n",
        ").to(device)\n",
        "\n",
        "#위의 모델 선언과 다른점은 위에서는 forward함수를 써서 이런식으로 순방향으로 계산하라고했지만,\n",
        "# 여기 sequential 모델은 말그대로 순차적으로 전달을 하라는 기본전제가 깔\n",
        "model_0"
      ],
      "metadata": {
        "id": "vqVStnnD9d-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "\n",
        "with torch.inference_mode():\n",
        "  untrained_preds=model_0(X_test.to(device))\n",
        "print(f\"Length of predictions: {len(untrained_preds)}, shape: {untrained_preds.shape}\")\n",
        "print(f\"Length of test samples: {len(X_test)}, Shape: {X_test.shape}\")\n",
        "print(f\"\\n First 10 predictions:\\n {torch.round(untrained_preds[:10])}\")\n",
        "print(f\"\\nFirst 10 labels:\\n{y_test}\")"
      ],
      "metadata": {
        "id": "MrKICe9fcyIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Setup loss function and optimizer\n",
        "\n",
        "Which loss function or optimizer should you use?\n",
        "\n",
        "Again... this is problem specific.\n",
        "\n",
        "For example for regression you might want MAE OR MSE(mean absolute error or mean squared error).\n",
        "\n",
        "For classification you might want binary cross entropy or categorical cross entropy(cross entropy).\n",
        "\n",
        "As a reminder, the loss function measures how *wrong* your model predictions are.\n",
        "\n",
        "And for optimizers, two of the most common and useful are SGD and Adam, However Pytorch has many built-in options.\n",
        "\n",
        "* For some common choice of loss functions and optimizers - https://www.learnpytorch.io/02_pytorch_classification/#21-setup-loss-function-and-optimizer\n",
        "\n",
        "* For the loss function we're going to use `torch.nn.BECWithLogitsLoss()`\n",
        "\n",
        "* For different optimizers see `torch.optim`"
      ],
      "metadata": {
        "id": "MHX4d48HUB-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.learnpytorch.io/02_pytorch_classification/"
      ],
      "metadata": {
        "id": "h06gjNZK1ER6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the loss function\n",
        "#loss_fn=nn.BCELoss() # BCELoss=requires inputs to have gone through the sigmoid activation prior to input to BCELoss(BCELoss에 들어오기전에 시그모이드를 거쳐야한다.)\n",
        "nn.Sequential(\n",
        "    nn.Sigmoid(),\n",
        "    nn.BCELoss()\n",
        ") # 이것이 BCEWithLogitsLoss()와 같다.\n",
        "loss_fn =nn.BCEWithLogitsLoss() # BCEWithLogitsLoss =sigmoid activation function built-in\n",
        "#위 손실함수가 더 수치적으로 안정적이다.\n",
        "optimizer=torch.optim.SGD(params=model_0.parameters(),\n",
        "                         lr=0.1)\n"
      ],
      "metadata": {
        "id": "1pxQZcCSzUjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "9yfQlkmSKj_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy - out of 100 examples, what percentage does our model get right?\n",
        "\n",
        "def accuracy_fn(y_true,y_pred):\n",
        "  correct=torch.eq(y_true,y_pred).sum().item()\n",
        "  acc=(correct/len(y_pred))*100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "7MmW0X_oNJWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train model\n",
        "\n",
        "To train our model, we're going to nee to build training loop following steps:\n",
        "\n",
        "1. Forward pass\n",
        "2. Calculate the loss\n",
        "3. Optimizer zero grad\n",
        "4. Loss backward\n",
        "5. optimizer step(gradient descent)"
      ],
      "metadata": {
        "id": "cxNTsoRaN__Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Going from raw logits -> prediction probabilities -> prediction labels\n",
        "\n",
        "our model outputs are going to be raw **logits**.\n",
        "\n",
        "로짓은 모델의 출력이며 모델 레이어의 순방향 기능에서 나온 것이다.\n",
        "\n",
        "We can convert these **logits** into **prediction probabilities** by passing them to\n",
        "some kind of **activation function** (e.g. sigmoid for binary crossentropy and softmax for multiclass classification) 시그모이드와 소프트맥스와 같은 활성화 함수를 통해 예측확률로 전환한다.\n",
        "\n",
        "Then we can convert our model's prediction probabilities to **Prediction label**\n",
        "by either rounding them or taking the `argmax()`\n",
        "\n",
        "`torch.round()`를 이용하여 label로 변환하고 argmax를 통해 높은 확률의 인자를 추출"
      ],
      "metadata": {
        "id": "IKy69a0pQFnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 5 outputs of the forward pass on the test data\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "  y_logits=model_0(X_test.to(device))\n",
        "y_logits[:,0]"
      ],
      "metadata": {
        "id": "OgCeXoQsGOSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test\n",
        "# y_logits를 y_test와 같이 형식을 바꿔주어야함\n"
      ],
      "metadata": {
        "id": "QjgLDZrlMrJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the sigmoid activation function on our model logits to turn them into prediction probabilities\n",
        "y_pred_probs=torch.sigmoid(y_logits)"
      ],
      "metadata": {
        "id": "0inkcs_x5GXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our prediction probabilities values, we need to perform a range-style rounding on them\n",
        "\n",
        "* `y_pred_probs` >=0.5 ,`y=1` (class 1)\n",
        "* `y_pred_probs` <0.5, `y=0 `(class 0)"
      ],
      "metadata": {
        "id": "l1LL-VI-ALX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the predicted labels\n",
        "y_preds=torch.round(y_pred_probs)\n",
        "\n",
        "# in full (raw logits(포워드패스를 통한)-> pred_probs(2진 분류: Sigmoid) -> pred_labels(torch.round를 통한 label작업)\n",
        "y_pred_labels=torch.round(torch.sigmoid(model_0(X_test.to(device))))\n",
        "\n",
        "# Check for equality\n",
        "print(torch.eq(y_preds.squeeze(),y_pred_labels.squeeze()))\n",
        "\n",
        "# Get rid of extra dimension\n",
        "y_preds.squeeze()"
      ],
      "metadata": {
        "id": "q679Ow2cAHfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Building a training and testing loop"
      ],
      "metadata": {
        "id": "c8oN6xXoCnEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn =nn.BCEWithLogitsLoss()\n",
        "optimizer=torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.01)"
      ],
      "metadata": {
        "id": "7I0yQe3MDEQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "ejpzTbGmRUOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs=1000\n",
        "Epoch=[]\n",
        "Loss_values=[]\n",
        "Test_Loss_values=[]\n",
        "# Put data to target device\n",
        "X_train,y_train=X_train.to(device),y_train.to(device)\n",
        "X_test,y_test=X_test.to(device),y_test.to(device)\n",
        "\n",
        "# Build training and evaluation loop\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_0.train()\n",
        "\n",
        "  # 1. Forward pass\n",
        "  y_logits=model_0(X_train).squeeze()\n",
        "  y_pred=torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
        "\n",
        "  # 2. Calculate loss/accuracy\n",
        "  loss=loss_fn(y_logits,\n",
        "               y_train) # BCEWithlogitsLoss는 이진분류일때 쓰임\n",
        "  acc=accuracy_fn(y_true=y_train,\n",
        "                  y_pred=y_pred)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Loss backward (backpropagation) 모든 매개변수에 대한 기울기를 계산합니다.\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Optimizer step (gradient descent) 기울기를 줄이기 위해 매개변수를 업데이트\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    # 1. Forward pass\n",
        "    test_logits=model_0(X_test).squeeze()\n",
        "    test_pred=torch.round(torch.sigmoid(test_logits))\n",
        "    # 2. Calculate loss / accuracy\n",
        "    test_loss=loss_fn(test_logits,y_test)\n",
        "    test_acc=accuracy_fn(y_true=y_test,\n",
        "                         y_pred=test_pred)\n",
        "\n",
        "\n",
        "  # Print out what's happening?\n",
        "  if epoch % 10==0:\n",
        "    Loss_values.append(loss)\n",
        "    Epoch.append(epoch)\n",
        "    Test_Loss_values.append(test_loss)\n",
        "    print(f\"Epoch:{epoch} | Loss: {loss:.5f}, ACC:{acc} % | Test loss: {test_loss:.5f}, Test acc:{test_acc} %\")"
      ],
      "metadata": {
        "id": "FB6h85xVC3nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "손실함수가 큰폭으로 떨어지지 않고, 정확도도 오히려 떨어짐..\n",
        "거의 학습을 하지않은 것과 유사 어떻게 고칠 수 있을까..\n"
      ],
      "metadata": {
        "id": "pGjHf7NIclVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make predictions and evaluate the model.\n",
        "\n",
        "From the metrics it looks like our model isn't learning anything...\n",
        "지표만 봤을때, 모델이 학습을 하지않는 것처럼 보인다.\n",
        "\n",
        "So to inspect it let's make some predictions and make them visual !\n",
        "이를 확인하기 위해선 예측을 하고 visual을 해야한다.\n",
        "In other words, Visualize, visualize, visualize\n",
        "\n",
        "TO do so, we're going to import a function called `plot_decision_boundary()` -\n",
        "https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py"
      ],
      "metadata": {
        "id": "CEnheLI4fL1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn Pytorch repo (if it's not already downloaded)\n",
        "if Path(\"helper_function.py\").is_file():\n",
        "  print(\"helper_function.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request=requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  #github의 Raw 버젼의 URL을 문자열로 입력해주어야함, 야 requests야 저 URL의 정보좀 가져와 (저 링크는 PYTHON 스크립트이다.)\n",
        "  with open(\"helper_functions.py\",\"wb\") as f: # \"helper_functions.py\"를 생성한 다음 \"WB\" 바이너리, f=\"helper_function.py\"가 될 것이다/\n",
        "    f.write(request.content) # f=\"helper_functions.py\"니까 저기 py스크립트에 request의 정보 즉, 저 URL의 스크립트 정보를 f에다가 입혀라\n",
        "from helper_functions import plot_predictions,plot_decision_boundary"
      ],
      "metadata": {
        "id": "C3cNspnll__N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundary of the model\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"train\")\n",
        "plot_decision_boundary(model_0,X_train,y_train)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"test\")\n",
        "plot_decision_boundary(model_0,X_test,y_test)"
      ],
      "metadata": {
        "id": "BXeA6ytGnq0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위에 보이는 선이 직선형태이다. 우리의 데이터는 원형인데 직선으로 빨간색점과 파란색점을 나눌 수 있을까..? Linear 선형 레이어를 짜서 이렇게 성능지표가 낮게 나온 건 아닐까 생각을 해보자.\n"
      ],
      "metadata": {
        "id": "vRFFN2w_oi4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(Test_Loss_values[0]),type(Test_Loss_values)\n",
        "\n",
        "# 외부 큰 틀은 list 형태인데, 리스트내의 원소는 torch.tensor인 형태\n",
        "Test_Loss_values=torch.stack(Test_Loss_values)\n",
        "Test_Loss_values.shape"
      ],
      "metadata": {
        "id": "SfHMZVww3RLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### can't convert to numpy 어쩌구 나오면 확인해보기 데이터 형태와 데이터 내의 원소 형태를"
      ],
      "metadata": {
        "id": "JkcUI2cP_Q86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.plot(Epoch,np.array(torch.tensor(Loss_values).numpy()),label=\"train loss\")\n",
        "plt.plot(Epoch,Test_Loss_values.cpu(), label=\"test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "\n",
        "#OVer fitting"
      ],
      "metadata": {
        "id": "fONnDbhpWlom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn_without_logits=nn.BCELoss()\n",
        "loss_fn_without_logits"
      ],
      "metadata": {
        "id": "F18xLUYxTlNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Improving a model (from a model perspective)\n",
        "\n",
        "* Add more layers - give the model more chances to learn about patterns in the data\n",
        "* Add more hidden units - go from 5 hiddne units to 10 hidden units # 모델이 데이터를 나타내는 매개 변수가 많아진다.\n",
        "* Fit for longer\n",
        "* Changing the activation functions 모델 내에 넣을 수 있는 활성화 함수도 존재한다.\n",
        "* Change the learning rate\n",
        "* Change the loss function\n",
        "\n",
        "These options are all from a model's perspective because they deal directly with the model, rather than the data\n",
        "위의 선택지들은 모두 모델의 관점으로부터 온 것이다 그 이유는 데이터를 건드리기 보다는 모델을 건드려서 성능을 올리려고 하는 것이기 때문이다.\n",
        "\n",
        "데이터의 관점에서 모델을 개선할 방법이 몇가지 더 있긴하다. 하지만 우리는 데이터를 개선하는 것보단 모델을 개선하는 것에 더 힘을 실을 것이다.\n",
        "\n",
        "**Because these options are all values we (as machine learning engineers and data scientists) can be change, they are referred as hyperparameters.**\n",
        "\n",
        "우리는 머신 러닝엔지니어로서 이러한 선택지들을 바꿀 수 있다. 그것들은 하이퍼 파라미터로 변경 가능 하기 때문에\n",
        "\n",
        "레이어 추가, 은닉노드 추가, 에포크 증가, 활성화기능 추가, 학습률 조정, 손실함수 변경 같은 것들은 우리가 하이퍼파라미터로 변경할 수 있기 때문에"
      ],
      "metadata": {
        "id": "h9gx96CQsEkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()\n",
        "\n",
        "# 20개쯤되는 파라미터들 0. 레이어의 파라미터들, 1. 레이어의 파라미터들\n",
        "# 만약 10개의 레이어가 있으면 데이터의 패턴을 파악할 매개변수가 10개는 더 늘어날 것이다.\n"
      ],
      "metadata": {
        "id": "Y_Sq68kNlU28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try and improve our model by:\n",
        "* Adding more hidden units: 5 -> 10\n",
        "* Increase the number of layers: 2 -> 3\n",
        "* Increase the number of epochs: 100 -> 1000\n",
        "\n",
        "여기선 3가지의 변수들을 모두 적용했지만, 실제로는 과학자들은 1가지의 변수들만 변경해본다. 통제할 수 있는 변수를 찾기 위해서\n"
      ],
      "metadata": {
        "id": "jh5oT-lTx1fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CircleModelV1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1=nn.Linear(in_features=2,out_features=10)\n",
        "    self.layer_2=nn.Linear(in_features=10,out_features=10)\n",
        "    self.layer_3=nn.Linear(in_features=10,out_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # 포워드의 방법중 1번째\n",
        "    # z=self.layer_1\n",
        "    # z=self.layer_2(z)\n",
        "    # z=self.layer_3(z)\n",
        "    # return z\n",
        "\n",
        "    # 포워드 방법 중 2번째\n",
        "    # 본 방법이 computing 연산에 있어서 빠르게 계산 가능\n",
        "    return self.layer_3(self.layer_2(self.layer_1(x)))\n",
        "model_1=CircleModelV1()\n",
        "model_1"
      ],
      "metadata": {
        "id": "dh6tSMHSxGvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.state_dict()"
      ],
      "metadata": {
        "id": "2YDRG4-9uezL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a loss function\n",
        "loss_fn=nn.BCEWithLogitsLoss()\n",
        "# Create an optimizer\n",
        "optimizer=torch.optim.SGD(params=model_1.parameters(),\n",
        "                          lr=0.1)"
      ],
      "metadata": {
        "id": "4Z4XHOd20dFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "def accuracy_fn(y_true,y_pred):\n",
        "> correct=torch.eq(y_true,y_pred).sum().item()\n",
        "\n",
        "> acc=(correct/len(y_pred))*100\n",
        "\n",
        "> return acc\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NB_XGKC2xfwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation loop for model_1\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Train for longer\n",
        "epochs=1000\n",
        "model_1.to(device)\n",
        "# Put data on the target device\n",
        "X_train,y_train=X_train.to(device),y_train.to(device)\n",
        "X_test,y_test=X_test.to(device),y_test.to(device)\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_1.train()\n",
        "  # 1. forward pass\n",
        "  y_logits=model_1(X_train).squeeze()\n",
        "  y_pred=torch.round(torch.sigmoid(y_logits)) # logits-> prediction probabilities -> labels\n",
        "\n",
        "  # 2.Calculate the loss\n",
        "  loss=loss_fn(y_logits,y_train) # y_pred 대신 y_logits을 쓰는 이유는 loss_fn이 BCEWithLogitsLoss()라서이다.\n",
        "  acc=accuracy_fn(y_true=y_train,y_pred=y_pred)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Loss backward (backpropagation) 모든 매개변수에 대한 기울기를 구한다.\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Optimizier step() 기울기를 줄이기 위해 매개변수를 업데이트\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_1.eval()\n",
        "  with torch.inference_mode():\n",
        "    # 1. forward pass\n",
        "    test_logits=model_1(X_test).squeeze()\n",
        "    test_pred=torch.round(torch.sigmoid(test_logits))\n",
        "    # 2. calculate loss\n",
        "    test_loss=loss_fn(test_logits, y_test)\n",
        "\n",
        "    test_acc=accuracy_fn(y_true=y_test,y_pred=test_pred)\n",
        "\n",
        "    # Print out what's happening\n",
        "    if epoch % 100 ==0:\n",
        "      print(f\"Epoch:{epoch} | Loss: {loss:.5f}. ACC:{acc:.2f}% | Test loss: {test_loss: .5f}, test acc:{test_acc:.5f}%\")"
      ],
      "metadata": {
        "id": "6tqbf1ja0j0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the decision boundary\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"train\")\n",
        "plot_decision_boundary(model_1,X_train,y_train)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"test\")\n",
        "plot_decision_boundary(model_1,X_test,y_test)"
      ],
      "metadata": {
        "id": "z90UC_8Dva8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "linear layer와 hidden unit 과 epoch 증가에도 성능은 여전\n",
        "코인던지기 처럼 선을 반반가르기해서 보여준다.\n",
        "비선형이필요\n"
      ],
      "metadata": {
        "id": "GhR2B7MH8Aa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Preparing data to see if our model can fit a straight line\n",
        "\n",
        "One way to troubleshoot to a larger problem is to test out a smaller problem\n",
        "\n",
        "위의 선형모델 v1을 선형데이터셋을 잠깐 구성해서 테스트해본다"
      ],
      "metadata": {
        "id": "Qzw5Q4HL8KNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create some data(same as notebook 01)\n",
        "\n",
        "weight=0.7\n",
        "bias=0.3\n",
        "start=0\n",
        "end=1\n",
        "step=0.01\n",
        "\n",
        "# Create data\n",
        "X_regression=torch.arange(start,end,step).unsqueeze(dim=1)\n",
        "y_regression=weight * X_regression + bias # Linear regression formula(without epsilon)\n",
        "\n",
        "# Check the data\n",
        "print(len(X_regression))\n",
        "X_regression[:5],y_regression[:5]"
      ],
      "metadata": {
        "id": "c3QRF0mtGNi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test split\n",
        "\n",
        "train_split =int(0.8 * len(X_regression))\n",
        "X_train_regression,y_train_regression= X_regression[:train_split], y_regression[:train_split]\n",
        "X_test_regression,y_test_regression=X_regression[train_split:],y_regression[train_split:]\n",
        "\n",
        "# Check length of data\n",
        "len(X_train_regression),len(y_train_regression),len(X_test_regression),len(y_test_regression)"
      ],
      "metadata": {
        "id": "8IxHUkWwG7Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train_regression[0]),type(X_train_regression),type(y_train_regression),type(y_train_regression[0]),type(X_test_regression),type(X_test_regression[0]),type(y_test_regression),type(y_test_regression[0])"
      ],
      "metadata": {
        "id": "HEYTs6qL9VKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "def plot_predictions(\n",
        "    train_data, train_labels, test_data, test_labels, predictions=None"
      ],
      "metadata": {
        "id": "7HqZlzlHPqRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(train_data=X_train_regression,\n",
        "                 train_labels=y_train_regression,\n",
        "                 test_data=X_test_regression,\n",
        "                 test_labels=y_test_regression)"
      ],
      "metadata": {
        "id": "4JNcwo_mHVmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 선형 데이터와 model_1가 적합할까?\n",
        "# 난 1번째 레이어의  in_feature를 1로 바꾸어야한다고 생각한다.\n",
        "# 그 이유는 X_train_regression의 입력값이 1개 이기때문이다.\n",
        "X_train_regression[0],model_1"
      ],
      "metadata": {
        "id": "uJKTmZ5zHVil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Adjust `model_1` to fit a straight line"
      ],
      "metadata": {
        "id": "tH8xkmbPQrD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Same architecture as model_1 (but using nn.Sequential())\n",
        "\n",
        "model_2=nn.Sequential(\n",
        "    nn.Linear(in_features=1,out_features=10),\n",
        "    nn.Linear(in_features=10,out_features=10),\n",
        "    nn.Linear(in_features=10,out_features=1)\n",
        ").to(device)\n",
        "model_2"
      ],
      "metadata": {
        "id": "Kt0HjqUIRGMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "loss_fn=nn.L1Loss()\n",
        "optimizer=torch.optim.SGD(params=model_2.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "KYILlpm_X8jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "wD46Ho-CASlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Train the model\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs=1000\n",
        "\n",
        "# Put the data on the target device\n",
        "X_train_regression,y_train_regression=X_train_regression.to(device),y_train_regression.to(device)\n",
        "X_test_regression,y_test_regression=X_test_regression.to(device),y_test_regression.to(device)\n",
        "\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "  y_pred=model_2(X_train_regression)\n",
        "  loss=loss_fn(y_pred,y_train_regression)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # testing\n",
        "  model_2.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_pred=model_2(X_test_regression)\n",
        "    test_loss=loss_fn(test_pred,y_test_regression)\n",
        "\n",
        "  # Print out what's happening\n",
        "  if epoch % 100==0:\n",
        "    print(f\"Epoch {epoch} | Loss:{loss:.5f} |Test_loss{test_loss:.5f}\")"
      ],
      "metadata": {
        "id": "o1EVRv-9auMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train_regression),"
      ],
      "metadata": {
        "id": "1ebqN0Vd9lXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Turn on evaluation mode\n",
        "model_2.eval()\n",
        "\n",
        "with torch.inference_mode():\n",
        "  y_preds=model_2(torch.Tensor(X_test_regression).to(device))\n",
        "\n",
        "# plot data and predictions\n",
        "# plot_prediction의 매개변수들은 cpu상에서 작업되어야한다. matplotlib은 cpu상인가보다..\n",
        "plot_predictions(train_data=X_train_regression.cpu(),\n",
        "                 train_labels=y_train_regression.cpu(),\n",
        "                 test_data=X_test_regression.cpu(),\n",
        "                 test_labels=y_test_regression.cpu(),\n",
        "                 predictions=y_preds.cpu())"
      ],
      "metadata": {
        "id": "5t3J5rS5oa-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. The missing piece: Non-linearity\n",
        "\n",
        "\"What patterns could you draw if you were given an infinite amount of a straight and non-straight line?\"\n",
        "\n",
        "Or in machine learning terms, and infinite (but really it is finite) of linear and non-linear functions?"
      ],
      "metadata": {
        "id": "yYqFwgRTnouH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Recreating non-linear data (red and blue circles)"
      ],
      "metadata": {
        "id": "l8N1mQYEohnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make and plot data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "n_samples=1000\n",
        "X,y=make_circles(n_samples,\n",
        "                 noise=0.03,\n",
        "                 random_state=42)\n",
        "\n",
        "plt.scatter(X[:,0],X[:,1],c=y,cmap=plt.cm.RdYlBu)"
      ],
      "metadata": {
        "id": "W6_NgJjgow5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#diagnostic code\n",
        "import torch\n",
        "from torch import nn\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "_Ooxfi-vFKDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true,y_pred):\n",
        "  correct=torch.eq(y_true,y_pred).sum().item()\n",
        "  acc=(correct/len(y_pred))*100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "3jtZjPFkF7CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to tensors and then to train and test split\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Turn data into tensors\n",
        "X=X\n",
        "#y=torch.from_numpy(y).type(torch.float)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,\n",
        "                                               y,\n",
        "                                               test_size=0.2,\n",
        "                                               random_state=42)\n",
        "\n",
        "X_train[:5],y_train[:5]"
      ],
      "metadata": {
        "id": "ddepcURVpVBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Building a model with non-linearity\n",
        "\n",
        "* Linear=straight line\n",
        "* Non linear= non-straight line\n",
        "\n",
        "Artificial neural networks are a large combination of linear (straight) and non_straight (non-linear) functions which are potentially able to find patterns in data.\n",
        "\n",
        "인공 신경망은 데이터의 패턴을 찾을 수 있는 선형과 비선형의 큰 조합이다.\n"
      ],
      "metadata": {
        "id": "lFH4N4M0pwGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a model with non-linear activation functions\n",
        "from torch import nn\n",
        "class CircleModelV2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer_1=nn.Linear(in_features=2,out_features=10)\n",
        "    self.layer_2=nn.Linear(in_features=10,out_features=10)\n",
        "    self.layer_3=nn.Linear(in_features=10,out_features=1)\n",
        "    self.relu=nn.ReLU() # Relu is a non linear activation function\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Where should we put our Non - linear activation functions?\n",
        "    return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n",
        "\n",
        "model_3=CircleModelV2().to(device)\n",
        "model_3"
      ],
      "metadata": {
        "id": "-x7QFwJzzCCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss and Optimizer\n",
        "\n",
        "loss_fn=nn.BCEWithLogitsLoss()\n",
        "optimizer=torch.optim.SGD(params=model_3.parameters(),\n",
        "                          lr=0.1)"
      ],
      "metadata": {
        "id": "Vo1zAY1wLVKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train),type(X_test),type(y_train),type(y_test)\n",
        "\n",
        "X_train=torch.Tensor(X_train).to(device)\n",
        "X_test=torch.Tensor(X_test).to(device)\n",
        "y_train=torch.Tensor(y_train).to(device)\n",
        "y_test=torch.Tensor(y_test).to(device)\n",
        "type(X_train),type(X_test),type(y_train),type(y_test)"
      ],
      "metadata": {
        "id": "ZcgjU-xreROa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Training a model with non-linearity"
      ],
      "metadata": {
        "id": "j5ouWsJQOhN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# set the number of epoch\n",
        "epochs=2000\n",
        "\n",
        "# Put all data on target device\n",
        "X_train,y_train=X_train.to(device),y_train.to(device)\n",
        "X_test,y_test=(X_test).to(device),y_test.to(device)\n",
        "Epoch=[]\n",
        "Loss_values=[]\n",
        "Test_Loss_values=[]\n",
        "print(X_train)\n",
        "# Training\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  ### Training\n",
        "  model_3.train()\n",
        "  # 1. Forward pass\n",
        "  y_logits=model_3(X_train).squeeze().to(device)\n",
        "  y_pred=torch.round(torch.sigmoid(y_logits))\n",
        "\n",
        "  # 2. Calculate the loss / acc\n",
        "  loss=loss_fn(y_logits,y_train) # BCEWithLogitsLoss 는 로짓이 인풋값\n",
        "  acc= accuracy_fn(y_true=y_train,\n",
        "                y_pred=y_pred)\n",
        "\n",
        "  # 3. Opitimzer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Loss backward\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Optimizer step()\n",
        "  optimizer.step()\n",
        "\n",
        "# TESTING\n",
        "  model_3.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_logits=model_3(X_test).squeeze()\n",
        "    test_pred=torch.round(torch.sigmoid(test_logits))\n",
        "\n",
        "    test_loss=loss_fn(test_logits,y_test)\n",
        "    test_acc=accuracy_fn(y_true=y_test,\n",
        "                         y_pred=test_pred)\n",
        "\n",
        " # print out what's happening\n",
        "  if epoch % 100==0:\n",
        "    Loss_values.append(loss)\n",
        "    Epoch.append(epoch)\n",
        "    Test_Loss_values.append(test_loss)\n",
        "    print(f\"Epoch {epoch} | Loss:{loss:.5f}, ACC={acc:.2f} |Test_loss{test_loss:.5f}, Test_acc: {test_acc:.2f}\")"
      ],
      "metadata": {
        "id": "UmfOihT1AWIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_3.state_dict()\n",
        "# Activation function 활성화함수에는 매개변수가 존재하지않음"
      ],
      "metadata": {
        "id": "4TGUqt9XVQVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train))"
      ],
      "metadata": {
        "id": "TH6jl7-PPloT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_logits.shape,y_train.shape"
      ],
      "metadata": {
        "id": "2CAtOCoMM0qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Evaluating a model trained with non-linear activation functions"
      ],
      "metadata": {
        "id": "SjvAagmfEaui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "model_3.eval()\n",
        "with torch.inference_mode():\n",
        "  y_preds=torch.round(torch.sigmoid(model_3(X_test))).squeeze()\n",
        "y_preds[:10],y_test[:10]"
      ],
      "metadata": {
        "id": "9-f17hTGHBwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "plt.plot(Epoch,np.array(torch.tensor(Loss_values).numpy()),label=\"train loss\")\n",
        "plt.plot(Epoch,Test_Loss_values, label=\"test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "EEAK1mGStjOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot decision boundaries\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,3,1)\n",
        "plt.title(\"Train\")\n",
        "plot_decision_boundary(model_3,X_train,y_train)\n",
        "plt.subplot(1,3,2)\n",
        "plt.title(\"Test\")\n",
        "plot_decision_boundary(model_3,X_test,y_test) # model_3 는 비선형적\n",
        "plt.subplot(1,3,3)\n",
        "plt.title(\"Model_1_train\")\n",
        "plot_decision_boundary(model_1,X_train,y_train) # Model_1 은 선형적"
      ],
      "metadata": {
        "id": "rTQq7PTxHV5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Replicating non-linear activation functions\n",
        "\n",
        "Neural networks, rather than us telling the model what to learn, we give it the tools to discover pattern in data and it tries to figure out the patterns its own\n",
        "\n",
        "인공신경망은 우리가 모델에게 어떤 것을 배우라고 말하기보단, 우리가 데이터내에 패턴을 발견할 툴을 주고 스스로 패턴을 찾으려고 노력한다.\n",
        "\n",
        "And these tools are linear & non-linear functions\n"
      ],
      "metadata": {
        "id": "9trdexnfJZX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor\n",
        "\n",
        "A=torch.arange(-10,10,1,dtype=torch.float32)"
      ],
      "metadata": {
        "id": "1wTgZWCyKOPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the tensor\n",
        "plt.plot(A)"
      ],
      "metadata": {
        "id": "Qz1gbBC7K7Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.relu(A))"
      ],
      "metadata": {
        "id": "egTdoO6lLHNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x: torch.Tensor) -> torch.Tensor:\n",
        "  return torch.maximum(torch.tensor(0),x) # 입력값은 무조건 텐서여야한다.\n",
        "\n",
        "relu(A)"
      ],
      "metadata": {
        "id": "wfcFxYARLQ9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ReLU activation function\n",
        "plt.plot(relu(A));"
      ],
      "metadata": {
        "id": "havX9zNcMDHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's do the same for sigmoid\n",
        "def sigmoid(x):\n",
        "  return 1/(1+torch.exp(-x))"
      ],
      "metadata": {
        "id": "HG_IlxaMMNsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.sigmoid(A))"
      ],
      "metadata": {
        "id": "jzBeZLXrMh6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.sigmoid(torch.relu(A)))"
      ],
      "metadata": {
        "id": "ZuJ35QF7TW0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(torch.relu(torch.sigmoid(A)))"
      ],
      "metadata": {
        "id": "ZgM89TNgTeu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기까지 이진 분류에대해서 얘기했다. 다음은 다중 분류에대해서 진행\n"
      ],
      "metadata": {
        "id": "XdahvjshOBeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Putting it all together with a multi-class classification problem\n",
        "\n",
        "* Binary classification = one thing or another(cat vs dog, spam vs not spam, fraud or not fraud)\n",
        "\n",
        "* Multi-class classification = more than one thing or another (cat vs dog vs chicken)"
      ],
      "metadata": {
        "id": "OxQECbixOGso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use activation functions with Softmax for Multi-classification and Cross EntropyLoss rather than BCELoss"
      ],
      "metadata": {
        "id": "ekLl6VcWT8Ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Creating a toy multi-class dataset\n"
      ],
      "metadata": {
        "id": "aDNeXEMmUDGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs  # https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# centers=10이면 10개의 클라스를 생성가능\n",
        "# 이진 분류를 원하면 centers=2 를 넣으면된다.\n",
        "\n",
        "# Set the hyperparameters for data creation\n",
        "NUM_CLASSES=4\n",
        "NUM_FEATURES=2\n",
        "RANDOM_SEED=42\n",
        "\n",
        "# 1. Create multi-class data\n",
        "\n",
        "X_blob,y_blob=make_blobs(n_samples=1000,\n",
        "                         n_features=NUM_FEATURES,\n",
        "                         centers=NUM_CLASSES,\n",
        "                         cluster_std=2, # 흩어짐의 정도를 주어 모델을 좀 힘들게 하겠다의 척도\n",
        "                         random_state=RANDOM_SEED\n",
        "                         )\n",
        "\n",
        "# 2. Turn data into Tensors\n",
        "type(X_blob),type(y_blob) #만들어진 데이터의 타입이 어떻게되는지 파악\n",
        "\n",
        "X_blob=torch.from_numpy(X_blob).type(torch.float) # 데이터 내부 원소도 torch.float로 변환\n",
        "y_blob=torch.from_numpy(y_blob).type(torch.LongTensor) # 데이터 내부원소도 torch.float로 변환  토치의 데이터기본형은 float32\n",
        "type(X_blob),X_blob.dtype\n",
        "\n",
        "# 3. Split into train and test\n",
        "X_blob_train,X_blob_test,y_blob_train,y_blob_test=train_test_split(X_blob,\n",
        "                                                                   y_blob,\n",
        "                                                                   test_size=0.2,\n",
        "                                                                   random_state=RANDOM_SEED)\n",
        "# 4. Plot data (visualize,visualize,visualize)\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.scatter(X_blob[:,0],X_blob[:,1],c=y_blob,cmap=plt.cm.RdYlBu)"
      ],
      "metadata": {
        "id": "5SgsnkznTAcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Building a multi-class classification model in Pytorch"
      ],
      "metadata": {
        "id": "w2-4c5gO3JG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create device agnostic code\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "nrsKjWBV3RAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a multi-class classification model\n",
        "class BlobModel(nn.Module):\n",
        "  def __init__(self,input_features,output_features,hidden_units=8):\n",
        "    \"\"\"Initialize Multi-class classification model\n",
        "\n",
        "    Args:\n",
        "      input_features(int): Number of input features to the model\n",
        "      output_features(int): Number of outputs features (number of output classes)\n",
        "      hidden_units (int): Number of hidden units between layers, default 8\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Example:\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.linear_layer_stack=nn.Sequential(\n",
        "        nn.Linear(in_features=input_features,out_features=hidden_units),\n",
        "        #nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units,out_features=hidden_units),\n",
        "        #nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units,out_features=output_features)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.linear_layer_stack(x)\n",
        "\n",
        "# Create an instance of BlobModel and send it to the target device\n",
        "model_4 = BlobModel(input_features=2,\n",
        "                    output_features=4,\n",
        "                    hidden_units=8).to(device)\n",
        "model_4"
      ],
      "metadata": {
        "id": "6wQ01oek8jhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_blob_train.shape,y_blob_train.shape\n",
        "len(y_blob_train),torch.unique(y_blob_train) # unique란 내부의 원소들을 중복없이 크기순서대로 나열"
      ],
      "metadata": {
        "id": "B8S4AKOs-CX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Create a Loss function and an Optimizer for multi-class classification model"
      ],
      "metadata": {
        "id": "3xwNvcmQSTVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a loss function for multi-class classification - loss function measures how wrong our model's predictions are\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "\n",
        "# Create an optimizer for multi-class classification - Optimizer updates our model parameters to try and reduce the loss\n",
        "optimizer=torch.optim.SGD(params=model_4.parameters(),\n",
        "                          lr=0.1) # learning rate is a hyperparameter you can change"
      ],
      "metadata": {
        "id": "HJI9nmKyTjWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 Getting prediction probabilities for a multi-class Pytorch Model\n",
        "\n",
        "In order to evaluate and train and test or model, we need to convert our model's output(logits) to prediction probabilities and then to prediction label.\n",
        "\n",
        "Logits (raw output of the model, `model_4(X_train)`) -> pred_probs (use ` torch.softmax(y_logits,dim=1)`) -> pred_labels (use `torch.argmax(y_pred_probs,dim=1)`)"
      ],
      "metadata": {
        "id": "aziuXwRSVkMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **자기의 모델과 데이터가 지금 어디 장치에 있는지 확인하는방법!!!!!!!!!!!!!**\n"
      ],
      "metadata": {
        "id": "3fYI9uDDWw1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next(model_4.parameters()).device, X_blob_test.device"
      ],
      "metadata": {
        "id": "_365T2i4W0jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get some raw outputs of our model called logits\n",
        "model_4.eval()\n",
        "with torch.inference_mode():\n",
        "  y_logits=model_4(X_blob_test.to(device))\n",
        "\n",
        "y_logits[:10]"
      ],
      "metadata": {
        "id": "l2qAPicqWYtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_blob_test[:10]"
      ],
      "metadata": {
        "id": "8VqgazLbrjdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert our model's logit outputs to prediction probabilities (로짓을 확률로 바꿔주기)\n",
        "y_pred_probs=torch.softmax(y_logits,dim=1) # dim=1 인 이유는 세로줄에 있는 것들을 더하라고 인것 같다. (1행 1열 + 1행2열 + 1행3열 + 1행4열)\n",
        "print(y_logits[:5])\n",
        "print(y_pred_probs[:5])"
      ],
      "metadata": {
        "id": "ljz1sEUQslTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  return torch.exp(x)/torch.sum(torch.exp(x))\n",
        "  # 소프트맥스는 해당 원소내의 있는 것들을 예측 확률로 변환해준다."
      ],
      "metadata": {
        "id": "2LTXu4v2tHTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(y_pred_probs[0])\n",
        "# 2번 인덱스가 가장 확률이 높아요 이런"
      ],
      "metadata": {
        "id": "RlceFz44tvUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conver our model's prediction probabilities to prediction labels (확률을 라벨로 변환하기)\n",
        "y_preds=torch.argmax(y_pred_probs,dim=1)\n",
        "y_preds"
      ],
      "metadata": {
        "id": "FVXeitvg0ffW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds.shape,y_blob_test.shape"
      ],
      "metadata": {
        "id": "YDR76kzF06Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_blob_train,y_blob_train=X_blob_train.to(device),y_blob_train.to(device)\n",
        "X_blob_test,y_blob_test=X_blob_test.to(device),y_blob_test.to(device)"
      ],
      "metadata": {
        "id": "tkNNbxxcJ8fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_logits=model_4(X_blob_train)"
      ],
      "metadata": {
        "id": "SkfswQXr_gFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_logits[0]"
      ],
      "metadata": {
        "id": "YFk_EAHU_jz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5 Creating a training loop and testing loop for a multi-class pytorch model"
      ],
      "metadata": {
        "id": "tLhNZcp91yad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true,y_pred):\n",
        "  correct=torch.eq(y_true,y_pred).sum().item()\n",
        "  acc=(correct/len(y_pred))*100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "P1U5WBw-AMLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_blob_train[:10],y_logits"
      ],
      "metadata": {
        "id": "YoNnkvzOATOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_blob_train=y_blob_train.type(torch.LongTensor)\n",
        "y_logits.dtype,y_blob_train.dtype"
      ],
      "metadata": {
        "id": "U_Y4sGIJKOlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the mulit-class model to the data\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs=100\n",
        "\n",
        "# Put data to the target device\n",
        "X_blob_train,y_blob_train=X_blob_train.to(device),y_blob_train.to(device)\n",
        "X_blob_test,y_blob_test=X_blob_test.to(device),y_blob_test.to(device)\n",
        "\n",
        "# Loop through data\n",
        "for epoch in range(epochs):\n",
        "  ## Training\n",
        "  model_4.train()\n",
        "  y_logits=model_4(X_blob_train)\n",
        "  y_pred=torch.softmax(y_logits,dim=1).argmax(dim=1)\n",
        "\n",
        "  loss=loss_fn(y_logits,y_blob_train.type(torch.LongTensor))# torch.Longtensor 는 cpu에서 작동, torch.cuda.LongTensor는 gpu에서 작동)\n",
        "  acc=accuracy_fn(y_true=y_blob_train,\n",
        "                  y_pred=y_pred)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  ### Testing\n",
        "  model_4.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_logits=model_4(X_blob_test)\n",
        "    test_preds=torch.softmax(test_logits,dim=1).argmax(dim=1)\n",
        "\n",
        "    test_loss=loss_fn(test_logits,y_blob_test)  # 크로스 엔트로피는 정수형...\n",
        "    test_acc=accuracy_fn(y_true=y_blob_test,\n",
        "                         y_pred=test_preds)\n",
        "\n",
        "\n",
        "  # Print out what's happening\n",
        "  if epoch % 10 ==0:\n",
        "    print(f\"Epoch:{epoch} | Loss:{loss:.4f} acc:{acc:.2f} | Test loss {test_loss:.4f}, Testacc:{test_acc:.2f}\")"
      ],
      "metadata": {
        "id": "dt397K1734yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_logits.device,y_blob_test.device"
      ],
      "metadata": {
        "id": "x64bnJtgBhU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.6 Making and evaluating predictions with a Pytorch multi-class model"
      ],
      "metadata": {
        "id": "hxlZ7ZYXfJCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "model_4.eval()\n",
        "with torch.inference_mode():\n",
        "  y_logits=model_4(X_blob_test)\n",
        "\n",
        "# View the first 10 predictions\n",
        "y_logits[:10]"
      ],
      "metadata": {
        "id": "O7aLuo8gfOUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go from logits -> prediction probabilities\n",
        "y_pred_probs=torch.softmax(y_logits,dim=1)\n",
        "y_pred_probs[:10]"
      ],
      "metadata": {
        "id": "V6Z9fRUxffo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go from pred probs to pred labels\n",
        "y_preds=torch.argmax(y_pred_probs,dim=1)\n",
        "y_preds[:10]"
      ],
      "metadata": {
        "id": "PXEHd1wqflm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"train\")\n",
        "plot_decision_boundary(model_4,X_blob_train,y_blob_train)\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"test\")\n",
        "plot_decision_boundary(model_4,X_blob_test,y_blob_test)"
      ],
      "metadata": {
        "id": "PBa8xShrfljr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. A few more classification metrics(평가지표) (to evaluate our classification model)\n",
        "\n",
        "* Accuracy - out of 100 samples, how many does our model get right?\n",
        "* Precision\n",
        "* Recall\n",
        "* F1-score\n",
        "* Cnfusion Matrix\n",
        "* Classification report\n",
        "\n",
        "https://torchmetrics.readthedocs.io/en/latest/pages/quickstart.html"
      ],
      "metadata": {
        "id": "EDU1OpY3hKd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "cU0sO-3oufJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import Accuracy\n",
        "\n",
        "# Setup metric (메트릭스도 diagnostic code가 있어야한다.)\n",
        "torchmetric_accuracy=Accuracy('multiclass').to(device)\n",
        "\n",
        "# Calculate accuracy"
      ],
      "metadata": {
        "id": "WMZZG69QqIxO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}