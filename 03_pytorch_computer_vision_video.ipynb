{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqdE3oY2YftslRGIuQxMBC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youse0ng/pytorch_practice/blob/main/03_pytorch_computer_vision_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Pytorch Computer Vision\n",
        "\n",
        "* See reference online book - https://www.learnpytorch.io/03_pytorch_computer_vision/\n",
        "\n",
        "* https://github.com/mrdbourke/pytorch-deep-learning/blob/main/helper_functions.py"
      ],
      "metadata": {
        "id": "LHFlI33B4V93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 0. Computer vision libraries in Pytorch\n",
        "\n",
        "* `torchvision` - base domain library for Pytorch computer vision\n",
        "* `torchvision.datasets` - get datasets and data loading functions for computuer vision here\n",
        "* `torchvision.models` - get pretrained computer vision models that you can leverage for your own problems\n",
        "* `torchvision.transform` - functions for manipulating your vision data (images) to be suitable foruse with an ML model\n",
        "* `torch.utils.data.Dataset` - Base dataset class for Pytorch.\n",
        "* `torch.utils.data.DataLoader` - Creates a Python iterable over a dataset"
      ],
      "metadata": {
        "id": "9cNV2a7fMpfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Pytorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check versions\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "id": "Wr3LTuQWOapA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting a dataset\n",
        "\n",
        "Fashion Mnist : The dataset we'll be using from torchvision.datasets"
      ],
      "metadata": {
        "id": "fyYvvo4wPWt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Training data\n",
        "from torchvision import datasets\n",
        "train_data=datasets.FashionMNIST(\n",
        "    root=\"data \", # where to download data to?\n",
        "    train=True, # 학습 데이터가 필요한가?\n",
        "    download=True, # 다운로드 할까요?\n",
        "    transform=torchvision.transforms.ToTensor(), # 어떻게 데이터를 변환하고 싶은가요?\n",
        "    #ToTensor(): Convert a PIL(Python imaging Library) Image or numpy.ndarray to Tensor\n",
        "    target_transform=None, # Do we want to transform the label? (No) 어떻게 레이블을 변환하고 싶은가요?\n",
        ")\n",
        "test_data=datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None,\n",
        ")"
      ],
      "metadata": {
        "id": "JpHRL4zxqlvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ToTensor()`는\n",
        "\n",
        "Convert PIL Image or numpy.ndarray(H x W x C) (height width Channel)의 (0,255) 픽셀값들을 (C H W)로 범위는 0~ 1 사이로 변환"
      ],
      "metadata": {
        "id": "EmNe8VAy8_Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data),len(test_data),type(train_data),type(test_data)"
      ],
      "metadata": {
        "id": "NDCFqCsaqtQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See the first training example\n",
        "image,label=train_data[0] # 튜플 형식\n",
        "image,label"
      ],
      "metadata": {
        "id": "WCMpJvViqtPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 확인 방법 1\n",
        "class_names=train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "xfp_W9F19uqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 클래스 확인방법 2\n",
        "class_to_idx=train_data.class_to_idx\n",
        "class_to_idx"
      ],
      "metadata": {
        "id": "K5mbBot694Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets"
      ],
      "metadata": {
        "id": "ksa-Lm9z-GR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of our image\n",
        "print(f\"image.shape: {image.shape}, [Color_Channel, height, width]\")\n",
        "print(f\"image label:{class_names[label]}\")"
      ],
      "metadata": {
        "id": "FC3EnFbc-ghN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Visualizing our data"
      ],
      "metadata": {
        "id": "tdTAMoLI_e_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "image,label=train_data[0]\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "plt.imshow(image.squeeze())\n",
        "plt.title(label)\n",
        "# 매트플롯립은 높이와 너비만을 예상하므로 [28,28]로 바꾸어줘야한다. 단일 차원 제거."
      ],
      "metadata": {
        "id": "NvN4VLm7COBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(),cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "#plt.axis(False) 축 삭제."
      ],
      "metadata": {
        "id": "8E7SS9WyCOrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot more images\n",
        "torch.manual_seed(42)\n",
        "fig=plt.figure(figsize=(8,8))\n",
        "rows, cols=4,4\n",
        "for i in range(1,rows*cols+1):\n",
        "  random_idx=torch.randint(0,len(train_data),size=[1]).item()\n",
        "  img,label=train_data[random_idx]\n",
        "  fig.add_subplot(rows,cols,i)\n",
        "  plt.imshow(img.squeeze(),cmap=\"gray\")\n",
        "  plt.title(class_names[label])\n",
        "  plt.axis(False)\n"
      ],
      "metadata": {
        "id": "JaI6x5uPCOnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do you think these items of clothing coulde be modelled with pure linear lines or do you think we'll need non-linearities\n",
        "\n",
        "- 너가 이 데이터 봤을때 선형식이 필요한거같아 아닌거 같아?"
      ],
      "metadata": {
        "id": "qhMUTa3uDi5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare Dataloader\n",
        "\n",
        "Right now, our data is in the form of Pytorch Datasets\n",
        "지금 현재 데이터는 Pytorch 데이터 세트 형식이다.\n",
        "\n",
        "`DataLoader` turns our dataset into a Python iterable\n",
        "\n",
        "More specifically, we want to turn our data into batches (or mini-batches).\n",
        "\n",
        "Why would we do this?\n",
        "28 * 28 의 샘플 60000개를 한번에 볼 수 있다는건 메모리가 그만큼 상당히 필요해야한다.\n",
        "\n",
        "배치 사이즈와 미니배치를 하는 이유\n",
        "1. It is more computationally efficient, as in, your computing hardware may not be able to look (store in memory) at 60000 images in one hit. So we break it down to 32 images at a time (batch size of 32).\n",
        "2. It gives our neural network more chances to update its gradients per epoch."
      ],
      "metadata": {
        "id": "r76oLn1CDfb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 형식\n",
        "train_data,test_data"
      ],
      "metadata": {
        "id": "eXQf3dflCOly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE=32\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader=DataLoader(dataset=train_data,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            shuffle=True)\n",
        "\n",
        "test_dataloader=DataLoader(dataset=test_data,\n",
        "                           batch_size=BATCH_SIZE,\n",
        "                           shuffle=False) #테스트 데이터셋은 셔플을 안 시키는 것이 오히려 더 평가할때 쉽다.\n",
        "\n",
        "train_dataloader,test_dataloader"
      ],
      "metadata": {
        "id": "DYJN-oSBCPX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train_dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}...\")\n",
        "print(f\"Length of test_dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}...\")"
      ],
      "metadata": {
        "id": "WqIFAcDCCPWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out what's inside? the training dataloader\n",
        "train_features_batch, train_labels_batch=next(iter(train_dataloader)) # 안의 내용을 확인하기 위한 iter함수 사용 (32개의 텐서가 들어가있는 1batch)\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "id": "19Oy4VknSmGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample\n",
        "#torch.manual_seed(42)\n",
        "random_idx=torch.randint(0,len(train_features_batch),size=[1]).item()\n",
        "img,label=train_features_batch[random_idx],train_labels_batch[random_idx]\n",
        "plt.imshow(img.squeeze(),cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "print(f\"Image size: {img.shape}\")\n",
        "print(f\"Label:{label},label_size={label.shape}\")"
      ],
      "metadata": {
        "id": "R-jNpASeCPUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model 0: Build a baseline model\n",
        "\n",
        "When starting to build a series of machine learning modelling experiments, it's best practice to start with a baseline model\n",
        "기계학습 모델링을 세우기 시작할때, 가장 좋은 연습은 기본 모델부터 시작하는게 좋다.\n",
        "\n",
        "A baseline model is a simple model you will try and improve upon with subsequent models/experiments\n",
        "\n",
        "In other words: Start simply and add complexity when necessary\n",
        "\n",
        "간단하게 시작해서 복잡성을 추가하라 필요할 때\n",
        "\n"
      ],
      "metadata": {
        "id": "lM1ESNWsCPSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model=nn.Flatten()\n",
        "\n",
        "# Get a single sample\n",
        "x=train_features_batch[0]\n",
        "x.shape\n",
        "\n",
        "# Flatten the sample\n",
        "output=flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"shape before flattening :{x.shape} -> [color_channel,height,width]\")\n",
        "print(f\"shape after flattening:{output.shape} -> [color_channel,height * width]\")"
      ],
      "metadata": {
        "id": "P93uhcgOocRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class FashionMNISTModelV0(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape:int,\n",
        "               hidden_units:int,\n",
        "               output_shape:int):\n",
        "    super().__init__()\n",
        "    self.layer_stack=nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units),\n",
        "        nn.Linear(in_features=hidden_units,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "-No1_iUxvBz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Setup model with input parameters\n",
        "model_0=FashionMNISTModelV0(\n",
        "    input_shape=784, #this is 28*28\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names) # one for every class\n",
        ")\n",
        "model_0.to(\"cpu\")"
      ],
      "metadata": {
        "id": "Wb8OuBxZwLB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x=torch.rand([1,1,28,28])\n",
        "model_0(dummy_x)"
      ],
      "metadata": {
        "id": "XNkb4x1Aw5eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Setup loss, optimizer and evaluation metrics\n",
        "\n",
        "* Loss function - Since we're working with multi-class data, our loss function will be `nn.CrossEntropyLoss()`\n",
        "* Optimizer - our Optimizer `torch.optim.SGD()`\n",
        "* Evaluation metric - Since we're working on a classification problem , let's use accuracy as our evaluation metric."
      ],
      "metadata": {
        "id": "fvvAkPguz738"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy (classfication metric)\n",
        "def accuracy_fn(y_true,y_pred):\n",
        "  correct=torch.eq(y_true,y_pred).sum().item()\n",
        "  acc=(correct/len(y_pred)) * 100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "8nKySxFWz71G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn Pytorch Repo\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download...\")\n",
        "else:\n",
        "  print(\"Downloading helper_function.py\")\n",
        "  request=requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "  with open(\"helper_functions.py\",\"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "tEVk85ZU1dLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy metric\n",
        "from helper_functions import accuracy_fn\n",
        "\n",
        "# Setup loss function and optimizer\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(params=model_0.parameters(),\n",
        "                          lr=0.1)"
      ],
      "metadata": {
        "id": "2FtHkHXF2j5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Creating a function to time our experiments\n",
        "\n",
        "Machine learning is very experimental.\n",
        "\n",
        "Two of the main things you'll often want to track are:\n",
        "1. Model's performance (loss and accuracy values etc)\n",
        "2. How fast it runs"
      ],
      "metadata": {
        "id": "QtC81I995Wv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "def print_train_time(start:float,\n",
        "                     end: float,\n",
        "                     device:torch.device=None):\n",
        "  \"\"\"Prints difference between start and end time.\"\"\"\n",
        "  total_time=end-start\n",
        "  print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "  return total_time"
      ],
      "metadata": {
        "id": "IgHx-DT553xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time=timer()\n",
        "# some code...\n",
        "end_time=timer()\n",
        "print_train_time(start_time,end=end_time,device=\"cpu\")"
      ],
      "metadata": {
        "id": "NHhKSDBs6v0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Creating a traning loop and training a model on batches of data\n",
        "\n",
        "Highlight that the optimizer will update a model's parameters once per batch rather than once per epoch...\n",
        "\n",
        "#배치마다 모델의 파라미터를 업데이트해준다는 점을 강조한다.\n",
        "\n",
        "1. Loop through epochs\n",
        "2. Loop through training batches, perform training steps, calculate the train loss *per batch*.\n",
        "3. Loop through testing batches, perform testing steps, calculate the test loss *per batch*.\n",
        "4. print out what's happening.\n",
        "5. Time it all (for fun).\n"
      ],
      "metadata": {
        "id": "odCZnk_jtgyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out enumerate\n",
        "for batch,(X,y) in enumerate(train_dataloader):\n",
        "  pass"
      ],
      "metadata": {
        "id": "9uCO_zz28UmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for X_test,y_test in enumerate(test_dataloader):\n",
        "  print(X_test)\n"
      ],
      "metadata": {
        "id": "7uUsqsVQ-S8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar.\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the seed and start timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu=timer()\n",
        "\n",
        "# set the number of epochs (we'll keep this small for faster training time)\n",
        "epochs=2\n",
        "\n",
        "# 전체 epoch loop 안에 batch loop가 있는 형\n",
        "\n",
        "# Create train and test loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n------\")\n",
        "  ### Training\n",
        "  train_loss=0 # Calculate the train loss per batch --> accumulate\n",
        "  # Add a loop to loop through the training batches\n",
        "  for batch,(X,y) in enumerate(train_dataloader): # enumerate 원소랑 인덱싱 번호를 추출.\n",
        "    model_0.train()\n",
        "    # 1. Forward pass\n",
        "    y_pred=model_0(X)\n",
        "    print(f\"batch={batch}\")\n",
        "    # 2. Calculate lsos(per batch)\n",
        "    loss=loss_fn(y_pred,y)\n",
        "    train_loss+=loss # accumulate train loss\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. loss backward\n",
        "    loss.backward() # 모델의 파라미터 기울기를 계산한다.\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step() # 가중치 업데이트\n",
        "\n",
        "    # Print out what's happening\n",
        "    if batch % 400==0:\n",
        "      print(f\"Looked at {batch * len(X)}/ {len(train_dataloader.dataset)} samples\") # batch : 1874 len(X):32\n",
        "\n",
        " # Divide total train loss by length of train dataloader\n",
        "  train_loss /=len(train_dataloader)\n",
        "  print(f\"train_loss={train_loss}\")\n",
        "  ### Testing\n",
        "  test_loss,test_acc=0,0\n",
        "  model_0.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test,y_test in test_dataloader: # test_dataloader에서는 왜 enumerate를 안할거지..? enumerate 가뭐지..-https://blog.naver.com/PostView.nhn?blogId=youndok&logNo=222053465832\n",
        "    # test_dataloader===> (각 32장의 인풋 이미지 텐서들, 인풋이미지의 클래스 넘버 텐서)가 튜플형태로 반환\n",
        "    # enumerate 를 안한 이유는 인덱싱번호가 생성이 되니까 그게아마 X_test에 반환이 되었을 거란 생각이든다. 그래서 model_0(X_test)에서 오류가 났을거란 생각이 듦. 그리고 y_test에는 (test_dataloader의 텐서와 텐서에 맞는 클래스 텐서의 튜플형태가 반환되었을것이다.)\n",
        "      # 1. Forward pass\n",
        "      test_pred=model_0(X_test)\n",
        "      # 2. Calculate loss (accumulatively)\n",
        "      test_loss += loss_fn(test_pred,y_test)\n",
        "      # 3. Calculate acc\n",
        "      test_acc+=accuracy_fn(y_true=y_test,y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "    # Calculate the test loss average per batch\n",
        "    test_loss /= len(test_dataloader)\n",
        "\n",
        "    # Calculate the test acc average per batch\n",
        "    test_acc/=len(test_dataloader)\n",
        "  # Print out what's happening\n",
        "  print(f\"\\n Train loss:{train_loss:.4f} | Test_loss: {test_loss:.4f}, Test acc: {test_acc:.4f}\")\n",
        "# Calculate training time\n",
        "train_time_end_on_cpu=timer()\n",
        "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
        "                                            end=train_time_end_on_cpu,\n",
        "                                            device=str(next(model_0.parameters()).device))\n"
      ],
      "metadata": {
        "id": "Vk_g_MyGt-tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(model_0.parameters()).device\n",
        "type(model_0.parameters())"
      ],
      "metadata": {
        "id": "0gDU7LpOFbht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generator 는 이터레이터를 생성해주는 함수이다.**\n",
        "\n",
        "-https://dojang.io/mod/page/view.php?id=2412\n",
        "\n",
        "함수안에 yield 를 선언하면 generator 로 바뀐다\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_7bHIkH441sJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make predictions and get model 0 results\n"
      ],
      "metadata": {
        "id": "KV-rUYJVNVOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "gNSimTMOUmGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model:torch.nn.Module,\n",
        "               data_loader:torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device=device):\n",
        "  \"\"\" Returns a dictionary containing the results of model predicting on data_loader.\"\"\"\n",
        "  loss, acc=0,0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X,y in tqdm(data_loader):\n",
        "      # Make our data device agnostic\n",
        "      X,y=X.to(device),y.to(device)\n",
        "      model=model.to(device)\n",
        "      # Make predictions\n",
        "      y_pred=model(X)\n",
        "      # accumulate the loss and acc values per batch\n",
        "      loss+=loss_fn(y_pred,y) #y_pred(logits)와 라벨링을 비교하여 loss 계산\n",
        "      acc+=accuracy_fn(y_true=y,\n",
        "                    y_pred=y_pred.argmax(dim=1))\n",
        "      # Scale loss and acc to find the average loss/acc per batch\n",
        "    loss /= len(data_loader)\n",
        "    acc /=len(data_loader)\n",
        "  return {\"model_name\": model.__class__.__name__, # only works when model was created with a class self.__class__.__name__를 참조하면 부모가 아닌 현재 클래스의 이름이 참조됩니다.\n",
        "          \"model_loss\":loss.item(),\n",
        "          \"model_acc\":acc}\n",
        "# calculate model 0 results on test dataset.\n",
        "model_0_results=eval_model(model=model_0,\n",
        "                           data_loader=test_dataloader, # test_dataloader 는 인제 ..generator 로써 안에 32장 인풋 텐서의 1000/32개의 배치들이 존재.\n",
        "                           loss_fn=loss_fn,\n",
        "                           accuracy_fn=accuracy_fn\n",
        "                           )\n",
        "model_0_results\n",
        "#print(len(X),len(test_dataloader),y)"
      ],
      "metadata": {
        "id": "BSVP1Hn7NdK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Setup device agnostic-code (for using a GPU if there is one)"
      ],
      "metadata": {
        "id": "6EeVltOwIu_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device-agnostic code\n",
        "import torch\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "CinN8TMTu5__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model 1: Building a better model with non-linearity\n",
        "\n",
        "We learned about the power of non-linearity in Notebook 02 -https://www.learnpytorch.io/02_pytorch_classification/#62-building-a-model-with-non-linearity"
      ],
      "metadata": {
        "id": "LyQ3_Mo3vgql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 내가만든 모델ㅋㅋㅋ\n",
        "from torch import nn\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape:int,\n",
        "               hidden_units:int,\n",
        "               output_shape:int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_Flatten=nn.Flatten(),\n",
        "    self.layer_Linear=nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units),\n",
        "    self.layer_Linear_last=nn.Linear(in_features=hidden_units,\n",
        "                  out_features=output_shape)\n",
        "    self.ReLU=nn.ReLU()\n",
        "  def forward(self,x):\n",
        "    return self.layer_Linear_last(self.ReLU(self.layer_Linear(self.ReLU(self.layer_Flatten(x)))))\n",
        "model_1=FashionMNISTModelV1(input_shape=784,\n",
        "                            hidden_units=10,\n",
        "                            output_shape=len(train_data.classes)).to(\"cpu\")\n",
        "model_1,model_1.state_dict()"
      ],
      "metadata": {
        "id": "VmeUJBj1mnQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model with non-linear and linear layers\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape:int,\n",
        "               hidden_units:int,\n",
        "               output_shape:int):\n",
        "    super().__init__()\n",
        "    self.layer_stack=nn.Sequential(\n",
        "        nn.Flatten(), # Flatten inputs into a single vector.\n",
        "        nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units, # 이전 아웃피쳐와 동일\n",
        "                  out_features=output_shape),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  def forward(self,x:torch.Tensor):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "nWYSqshs5M_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(model_0.parameters()).device"
      ],
      "metadata": {
        "id": "NuSWNvrDRn3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of model_1\n",
        "torch.manual_seed(42)\n",
        "model_1=FashionMNISTModelV1(input_shape=784,\n",
        "                            hidden_units=10,\n",
        "                            output_shape=len(class_names)\n",
        "                            ).to(device) # send to the GPU if it's available\n",
        "next(model_1.parameters()).device\n"
      ],
      "metadata": {
        "id": "ji5ZdCrnTWt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Set up loss, optimizer and evaluation metrics"
      ],
      "metadata": {
        "id": "TIMjKBmMTWnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "loss_fn=nn.CrossEntropyLoss() # measure how wrong our model is\n",
        "optimizer=torch.optim.SGD(params=model_1.parameters(), # tries to update our model's parameters to reduce the loss\n",
        "                          lr=0.1)"
      ],
      "metadata": {
        "id": "BTyyNDvGg13J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Functionizing training and evaluation/testing loops\n",
        "\n",
        "Let's create a function for:\n",
        "* training loop - `train_step()`\n",
        "* testing loop - `test_step()`"
      ],
      "metadata": {
        "id": "ec_iaXE5lWDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model:torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn:torch.nn.Module,\n",
        "               optimizer:torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device:torch.device=device\n",
        "               ):\n",
        "  \"\"\" Perform a training with model trying to learn on data_loader.\"\"\"\n",
        "  train_loss,train_acc=0,0\n",
        "\n",
        "  # put model into training mode\n",
        "  model.train()\n",
        "\n",
        "  # Add a loop to loop through the training batches\n",
        "  for batch,(X,y) in enumerate(data_loader): # enumerate 원소랑 인덱싱 번호를 추출.\n",
        "    # Put data on target device\n",
        "    X,y=X.to(device), y.to(device)\n",
        "\n",
        "    # 1. Forward pass\n",
        "    y_pred=model(X) # outputs the raw logits from the model\n",
        "    print(f\"batch={batch}\")\n",
        "\n",
        "    # 2. Calculate loss / acc(per batch)\n",
        "    loss=loss_fn(y_pred,y)\n",
        "    train_loss+=loss # accumulate train loss\n",
        "    train_acc+=accuracy_fn(y_true=y,\n",
        "                           y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    # 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. loss backward\n",
        "    loss.backward() # 모델의 파라미터 기울기를 계산한다.\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step() # 가중치 업데이트\n",
        "\n",
        " # Divide total train loss and acc by length of train dataloader\n",
        "  train_loss /=len(data_loader)\n",
        "  train_acc /=len(data_loader)\n",
        "  print(f\"Train Loss : {train_loss:.5f} | Train acc : {train_acc:.2f} %\")\n"
      ],
      "metadata": {
        "id": "TfhT1DH-lWrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Testing\n",
        "def test_step(model:torch.nn.Module,\n",
        "              loss_fn:torch.nn.Module,\n",
        "              test_dataloader:torch.utils.data.DataLoader,\n",
        "              accuracy_fn,\n",
        "              device:torch.device=device):\n",
        "  \"\"\" Peforms a testing loop step on model going over data_loader.\"\"\"\n",
        "  test_loss,test_acc=0,0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test,y_test in test_dataloader: # test_dataloader에서는 왜 enumerate를 안할거지..? enumerate 가뭐지..-https://blog.naver.com/PostView.nhn?blogId=youndok&logNo=222053465832\n",
        "    # test_dataloader===> (각 32장의 인풋 이미지 텐서들, 인풋이미지의 클래스 넘버 텐서)가 튜플형태로 반환\n",
        "    # enumerate 를 안한 이유는 인덱싱번호가 생성이 되니까 그게아마 X_test에 반환이 되었을 거란 생각이든다. 그래서 model_0(X_test)에서 오류가 났을거란 생각이 듦. 그리고 y_test에는 (test_dataloader의 텐서와 텐서에 맞는 클래스 텐서의 튜플형태가 반환되었을것이다.)\n",
        "      X_test,y_test=X_test.to(device),y_test.to(device)\n",
        "      # 1. Forward pass\n",
        "      test_pred=model(X_test)\n",
        "      # 2. Calculate loss (accumulatively)\n",
        "      test_loss += loss_fn(test_pred,y_test)\n",
        "      # 3. Calculate acc\n",
        "      test_acc+=accuracy_fn(y_true=y_test,y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "    # Calculate the test loss average per batch\n",
        "    test_loss /= len(test_dataloader)\n",
        "\n",
        "    # Calculate the test acc average per batch\n",
        "    test_acc/=len(test_dataloader)\n",
        "    print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f} %\")"
      ],
      "metadata": {
        "id": "HWktxBAhlWpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# CPU와 GPU의 시간 차이 측\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_on_gpu=timer()\n",
        "\n",
        "# Set epochs\n",
        "epochs=3\n",
        "\n",
        "# Create an optimization and evaluation loop using train_step() and test_step()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch} \\n-----------\")\n",
        "  train_step(model=model_1,\n",
        "            data_loader=train_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            accuracy_fn=accuracy_fn,\n",
        "            device=device)\n",
        "  test_step(model=model_1,\n",
        "            test_dataloader= test_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            accuracy_fn=accuracy_fn,\n",
        "            device=device)\n",
        "\n",
        "  train_time_end_on_gpu=timer()\n",
        "  total_train_time_model_1=print_train_time(start=train_time_start_on_gpu,\n",
        "                                            end=train_time_end_on_gpu,\n",
        "                                            device=device)"
      ],
      "metadata": {
        "id": "wQmL10HOlWnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Note:** sometimes, depending on your data / hardware you might find that your model trains faster on CPU than GPU\n",
        "\n",
        " why is this\n",
        "\n",
        " 1. It could be that the overhead for copying data / model to and from the GPU outweighs the compute benefits offered by the GPU.\n",
        " 2. The hardware you're using has a better CPU in terms compute capability than the GPU\n",
        "\n",
        " For more on how to make your models compute faster, see here:https://horace.io/brrr_intro.html\n"
      ],
      "metadata": {
        "id": "YLvkVWmFp3DU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_1 results dictionary\n",
        "model_1_results=eval_model(model=model_1,\n",
        "                          data_loader=test_dataloader,\n",
        "                          loss_fn=loss_fn,\n",
        "                          accuracy_fn=accuracy_fn)\n",
        "model_1_results,"
      ],
      "metadata": {
        "id": "-UladOuYp3B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Building a Convolutional Neural Network(CNN)\n",
        "\n",
        "CNN's are also known Convnets.\n",
        "CNN's are known for their capabilities to find patterns in visual data.\n",
        "To Find out what's happneing inside a CNN. https://poloclub.github.io/cnn-explainer/"
      ],
      "metadata": {
        "id": "RJTOCp4Xp2_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a convolutional neural network\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "  \"\"\"\n",
        "  Model architecture that replicates the TinyVGG\n",
        "  Model from CNN Explainer website.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_shape: int,hidden_units: int,output_shape:int):\n",
        "    super().__init__()\n",
        "    self.conv_block_1= nn.Sequential(\n",
        "        # Create a conv layer - https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2,2))\n",
        "    )\n",
        "    self.conv_block_2=nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  padding=1,\n",
        "                  stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.classifier=nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*7*7,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x=self.conv_block_1(x)\n",
        "    print(f\"output shape of conv_block_1: {x.shape}\")\n",
        "    x=self.conv_block_2(x)\n",
        "    print(f\"output shape of conv_block_2: {x.shape}\")\n",
        "    x=self.classifier(x)\n",
        "    print(f\"output shape of classifier : {x.shape}\")\n",
        "    return x"
      ],
      "metadata": {
        "id": "cCqiVFiyp26s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_2=FashionMNISTModelV2(input_shape=1, # 컬러채널\n",
        "                            hidden_units=10,\n",
        "                            output_shape=len(class_names)\n",
        "                            ).to(device)"
      ],
      "metadata": {
        "id": "uWWKrJYMJHP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "id": "YaHrgxCKJeU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.state_dict()"
      ],
      "metadata": {
        "id": "hvp6i1gu1Y60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(),cmap='gray')"
      ],
      "metadata": {
        "id": "lkNbYI-TMxmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Stepping through `nn.Conv2d()`\n"
      ],
      "metadata": {
        "id": "_sBolMy0v44Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a batch of images\n",
        "images=torch.randn(size=(32,3,64,64))\n",
        "test_image=images[0]\n",
        "print(f\"image batch shape:{images.shape}\")\n",
        "print(f\"SIngle image shape:{test_image.shape}\")\n",
        "print(f\"Test_image:\\n {test_image}\")"
      ],
      "metadata": {
        "id": "ZrxPtYbU02IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a single conv2d layer\n",
        "conv_layer=nn.Conv2d(in_channels=3, # 인채널은 컬러 채널\n",
        "                     out_channels=10,\n",
        "                     kernel_size=3,\n",
        "                     stride=1,\n",
        "                     padding=0\n",
        "                     )\n",
        "# Pass the data through the convolutional layer\n",
        "conv_output=conv_layer(test_image)\n",
        "conv_output"
      ],
      "metadata": {
        "id": "p5GQRdy71q7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Stepping through `nn.MaxPool2d()`\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html"
      ],
      "metadata": {
        "id": "bWK6HOJwX2qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image.shape"
      ],
      "metadata": {
        "id": "XDVyg2Nm9hmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out original image shape without unsqueezed dimension\n",
        "print(f\"Test image original shape: {test_image.shape}\")\n",
        "print(f\"test image with unsqueezed dimesion: {test_image.unsqueeze(0).shape}\")\n",
        "\n",
        "# Create a sample nn.MaxPool2d layer\n",
        "max_pool_layer=nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass data through just the conv_layer\n",
        "test_image_through_conv=conv_layer(test_image.unsqueeze(dim=0))\n",
        "print(f\"shape after going through conv_layer():{test_image_through_conv.shape}\")\n",
        "\n",
        "# Pass data through the max pool layer\n",
        "test_image_through_conv_and_max_pool=max_pool_layer(test_image_through_conv)\n",
        "print(f\"shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")"
      ],
      "metadata": {
        "id": "1c0TUJFstJYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SHIFT ctrl SPACE - docstring"
      ],
      "metadata": {
        "id": "8sQQINahddbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# Create a random tensor with a similar number of dimensions to our images\n",
        "random_tensor=torch.randn(size=(1,1,2,2))\n",
        "print(f\"Random tensor: \\n {random_tensor}\")\n",
        "print(f\"Randomw tensor shape: {random_tensor.shape}\")\n",
        "#Create maxpool layer\n",
        "max_pool_layer=nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass the random tensor through the maxpoollayer\n",
        "max_pool_tensor=max_pool_layer(random_tensor)\n",
        "print(f\"\\nMax pool tensor: \\n{max_pool_tensor}\")\n",
        "print(f\"Max pool tensor shape: \\n{max_pool_tensor.shape}\")"
      ],
      "metadata": {
        "id": "3EA7TQhd4s2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_image_tensor=torch.randn(size=(1,28,28))\n",
        "rand_image_tensor.shape"
      ],
      "metadata": {
        "id": "n4wvCUF_NRbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Setup Loss function and optimizer for model_2\n"
      ],
      "metadata": {
        "id": "Zemmf_AnFMaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up loss function/eval metrics/opitimizer\n",
        "from helper_functions import accuracy_fn\n",
        "\n",
        "loss_fn=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.SGD(params=model_2.parameters(),\n",
        "                          lr=0.1)\n"
      ],
      "metadata": {
        "id": "1eOQz5fRFd6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.state_dict()"
      ],
      "metadata": {
        "id": "SC2EjZktGDwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4 Training and testing `model_2`using our training and test functions"
      ],
      "metadata": {
        "id": "ls7bo4iHGNKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Measure time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_model_2 =timer()\n",
        "\n",
        "# Train and test model\n",
        "epochs=3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch : {epoch}\\n-----\")\n",
        "  train_step(model=model_2,\n",
        "             data_loader=train_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             accuracy_fn=accuracy_fn,\n",
        "             device=device)\n",
        "  test_step(model=model_2,\n",
        "            test_dataloader= test_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            accuracy_fn=accuracy_fn,\n",
        "            device=device)\n",
        "\n",
        "train_time_end_model_2=timer()\n",
        "total_train_time_model_2=print_train_time(start=train_time_start_model_2,\n",
        "                                          end=train_time_end_model_2,\n",
        "                                          device=device)\n",
        "\n"
      ],
      "metadata": {
        "id": "zUwk9w7sGNGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_2 results\n",
        "model_2_results=eval_model(\n",
        "    model=model_2,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "model_2_results"
      ],
      "metadata": {
        "id": "0dv0SWXWP067"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Compare model results and trianing time"
      ],
      "metadata": {
        "id": "3NFTl_ozQnxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "compare_results=pd.DataFrame([model_0_results,\n",
        "                              model_1_results,\n",
        "                              model_2_results])\n",
        "compare_results"
      ],
      "metadata": {
        "id": "jiMFZh9eQvic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add training time to results comparision\n",
        "compare_results[\"training_time\"]=[total_train_time_model_0,\n",
        "                                   total_train_time_model_1,\n",
        "                                   total_train_time_model_2]\n",
        "\n",
        "compare_results"
      ],
      "metadata": {
        "id": "PhFPPGwlRyIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize our model results\n",
        "compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\n",
        "plt.xlabel(\"accuracy (%)\")\n",
        "plt.ylabel(\"model\")"
      ],
      "metadata": {
        "id": "Wp5FbJ-1SxxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Make and evaluate random predictions with best model"
      ],
      "metadata": {
        "id": "B2A9sHEccEmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model:torch.nn.Module,\n",
        "                      data:list,\n",
        "                      device:torch.device=device):\n",
        "  pred_probs=[]\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  with torch.inference_mode():\n",
        "    for sample in data:\n",
        "    # Prepare the sample (add a batch dimension and pass to target device)\n",
        "      sample=torch.unsqueeze(sample,dim=0).to(device)\n",
        "\n",
        "    # Forward pass (model outputs raw logits)\n",
        "      pred_logit=model(sample)\n",
        "\n",
        "    # Get prediction probability (logit->prediction probability)\n",
        "      pred_prob=torch.softmax(pred_logit.squeeze(),dim=0)\n",
        "\n",
        "    # Get pred_prob off the GPU for further calculation\n",
        "      pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "  # Stack the pred_probs to turn list into a tensor\n",
        "  return torch.stack(pred_probs)"
      ],
      "metadata": {
        "id": "cRzs9UPecTJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(test_data)"
      ],
      "metadata": {
        "id": "ZRAW2gH9s2mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# random.seed(42)\n",
        "test_samples=[]\n",
        "test_labels=[]\n",
        "\n",
        "for sample,label in random.sample(list(test_data),k=9): #random.sample(list내부의 원소중 9개를 뽑)\n",
        "  test_samples.append(sample)\n",
        "  test_labels.append(label)\n",
        "  # View the first sample shape\n",
        "test_samples[0].shape"
      ],
      "metadata": {
        "id": "WAkd2G-bsGCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_samples[0].squeeze(),cmap=\"gray\")\n",
        "plt.title(class_names[test_labels[0]])"
      ],
      "metadata": {
        "id": "45aor31QsF7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "pred_probs=make_predictions(model=model_2,\n",
        "                            data=test_samples,\n",
        "                            device=device)\n",
        "\n",
        "# View first two prediction probabilties\n",
        "pred_probs[:2]"
      ],
      "metadata": {
        "id": "TqWnyZbsvBZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert prediction probabilities to labels\n",
        "pred_classes=pred_probs.argmax(dim=1)\n",
        "pred_classes"
      ],
      "metadata": {
        "id": "K0dPjyMawvKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels"
      ],
      "metadata": {
        "id": "vRcORHV6wmI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "plt.figure(figsize=(9,9))\n",
        "nrows=3\n",
        "ncols=3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  #Create subplot\n",
        "  plt.subplot(nrows,ncols, i+1)\n",
        "  print(i)\n",
        "  # Plot the target image\n",
        "  plt.imshow(sample.squeeze(),cmap=\"gray\")\n",
        "\n",
        "  # Find the prediction(in text form, e.g \"Sandal\")\n",
        "  pred_label=class_names[pred_classes[i]] # pred_label에 예측한 클래스의 이름이 담김\n",
        "\n",
        "  # Get the truth label(in text form)\n",
        "  truth_label=class_names[test_labels[i]] # 정답값인 클래시 이름이 담김\n",
        "\n",
        "  # Create a title for the plot\n",
        "  title_text=f\"pred:{pred_label} | Truth: {truth_label}\"\n",
        "\n",
        "  # Check for quality between pred and truth and change color of title text\n",
        "  if pred_label==truth_label:\n",
        "    plt.title(title_text,fontsize=10, c=\"g\") # green text if prediction same as truth\n",
        "  else:\n",
        "    plt.title(title_text,fontsize=10, c=\"r\") # 정답이 아니면 빨간\n",
        "\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "4343LWEWBDfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making a confusion matrix for further prediction evaluating\n",
        "\n",
        "A Confusion matrix is a fantastic way of evaluating your classification models visually:\n",
        "https://www.learnpytorch.io/02_pytorch_classification/#9-more-classification-evaluation-metrics\n",
        "\n",
        "1. Make predictions with our trained model on the test datasets.\n",
        "2.Make a confusion matrix `torchmetrics.ConfusionMatrix` -https://torchmetrics.readthedocs.io/en/stable/classification/confusion_matrix.html\n",
        "3. Plot the confusion matrix using `mlxtend.plotting.plot_confusion_matrix()` -https://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/"
      ],
      "metadata": {
        "id": "5WfgRTkc0sb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm.auto\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Make predictions with Trained model_2\n",
        "\n",
        "y_preds=[]\n",
        "model_2.eval()\n",
        "with torch.inference_mode():\n",
        "  for X,y in tqdm(test_dataloader, desc=\"Making predictions...\"):\n",
        "    # Send the data and targets to target device\n",
        "    X,y=X.to(device),y.to(device)\n",
        "\n",
        "    # Do the forward pass\n",
        "    y_logits=model_2(X)\n",
        "\n",
        "    # Turn predictions from logits -> Prediction probabilities -> prediction labels\n",
        "    y_pred=torch.softmax(y_logits.squeeze(),dim=0).argmax(dim=1) #, dim=0은 열 방향으로 연산을 수행하고 dim=1은 행 방향으로 연산을 수행합니다.\n",
        "\n",
        "    # Put prediction on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "\n",
        "# Concatenate list of predictions into a tensor\n",
        "print(y_preds)\n",
        "y_preds_tensor=torch.cat(y_preds)\n",
        "y_preds_tensor"
      ],
      "metadata": {
        "id": "2Pyq0sSd3YKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See if required packages are installed and if not, install them...\n",
        "\n",
        "try:\n",
        "  import torchmetrics,mlxtend\n",
        "  print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "  assert int(mlxtend.__version__.split(\".\")[1]>=19,\"mlxtend version should be 0.19.0 or higher\")\n",
        "except:\n",
        "  !pip install -q torchmetrics -U mlxtend\n",
        "  import torchmetrics,mlxtend\n",
        "  print(f\"mlxtend version:{mlxtend.__version__}\")"
      ],
      "metadata": {
        "id": "1-hdT3so3YGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlxtend\n",
        "mlxtend.__version__"
      ],
      "metadata": {
        "id": "qpdf52ho2kv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# 2. Setup confusion instance and compare predictions to targets\n",
        "\n",
        "confmat=ConfusionMatrix(task='multiclass',num_classes=len(class_names))\n",
        "confmat_tensor=confmat(preds=y_preds_tensor,\n",
        "                       target=test_data.targets)\n",
        "\n",
        "# 3.plot the confusion matrix\n",
        "fig , ax=plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with numpy\n",
        "    class_names=class_names,\n",
        "    figsize=(10,7)\n",
        ")"
      ],
      "metadata": {
        "id": "3xDXwPQQ2kt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Save and load Best performing model"
      ],
      "metadata": {
        "id": "oepKwwwBHHC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create model directory path\n",
        "MODEL_PATH=Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True,\n",
        "                 exist_ok=True)\n",
        "\n",
        "# Create model save\n",
        "MODEL_NAME= \"03_pytorch_computer_vision_model_2.pth\"\n",
        "MODEL_SAVE_PATH=MODEL_PATH/MODEL_NAME\n",
        "\n",
        "# Save the model state_dict\n",
        "print(f\"Saving model to : {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_2.state_dict(),\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "gHh0uB2jHa1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new instance\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "loaded_model_2=FashionMNISTModelV2(input_shape=1,\n",
        "                                   hidden_units=10,\n",
        "                                   output_shape=len(class_names))\n",
        "\n",
        "# Load in the save state_dict()\n",
        "loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n",
        "\n",
        "# Send the model to the target device\n",
        "loaded_model_2.to(device)"
      ],
      "metadata": {
        "id": "w1vRWuwNPEQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results"
      ],
      "metadata": {
        "id": "_xcaSQsKRaID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate loaded model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "loaded_model_2_results=eval_model(\n",
        "    model=loaded_model_2,\n",
        "    data_loader=test_dataloader,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn\n",
        ")\n",
        "loaded_model_2_results"
      ],
      "metadata": {
        "id": "etroK9eIRXRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if model results are close to each other\n",
        "torch.isclose(torch.tensor(model_2_results[\"model_loss\"]),\n",
        "              torch.tensor(loaded_model_2_results[\"model_loss\"]))"
      ],
      "metadata": {
        "id": "Wilm4ElNAVc-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}